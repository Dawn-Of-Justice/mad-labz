{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BarkModel, AutoProcessor\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 353/353 [00:00<?, ?B/s] \n",
      "c:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\salos\\.cache\\huggingface\\hub\\models--suno--bark-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "speaker_embeddings_path.json: 100%|██████████| 61.1k/61.1k [00:00<00:00, 4.06MB/s]\n",
      "vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.57MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.92M/2.92M [00:01<00:00, 2.63MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "pytorch_model.bin: 100%|██████████| 1.68G/1.68G [01:53<00:00, 14.8MB/s]\n",
      "c:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "c:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "generation_config.json: 100%|██████████| 4.91k/4.91k [00:00<?, ?B/s]\n",
      "The BetterTransformer implementation does not support padding during training, as the fused kernels do not support attention masks. Beware that passing padded batched data during training may result in unexpected outputs. Please refer to https://huggingface.co/docs/optimum/bettertransformer/overview for more details.\n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(\"suno/bark-small\")\n",
    "model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n",
    "model =  model.to_bettertransformer()\n",
    "\n",
    "model.enable_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_preset = \"v2/en_speaker_6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "en_speaker_6_semantic_prompt.npy: 100%|██████████| 2.60k/2.60k [00:00<?, ?B/s]\n",
      "c:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\salos\\.cache\\huggingface\\hub\\models--ylacombe--bark-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "en_speaker_6_coarse_prompt.npy: 100%|██████████| 7.55k/7.55k [00:00<?, ?B/s]\n",
      "en_speaker_6_fine_prompt.npy: 100%|██████████| 15.0k/15.0k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(\"Hello, my dog is cute\", voice_preset=voice_preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:10000 for open-end generation.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m audio_array \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      2\u001b[0m audio_array \u001b[38;5;241m=\u001b[39m audio_array\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[1;32mc:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:1829\u001b[0m, in \u001b[0;36mBarkModel.generate\u001b[1;34m(self, input_ids, history_prompt, return_output_lengths, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m semantic_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1822\u001b[0m     input_ids,\n\u001b[0;32m   1823\u001b[0m     history_prompt\u001b[38;5;241m=\u001b[39mhistory_prompt,\n\u001b[0;32m   1824\u001b[0m     semantic_generation_config\u001b[38;5;241m=\u001b[39msemantic_generation_config,\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_semantic,\n\u001b[0;32m   1826\u001b[0m )\n\u001b[0;32m   1828\u001b[0m \u001b[38;5;66;03m# 2. Generate from the coarse model\u001b[39;00m\n\u001b[1;32m-> 1829\u001b[0m coarse_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoarse_acoustics\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1830\u001b[0m     semantic_output,\n\u001b[0;32m   1831\u001b[0m     history_prompt\u001b[38;5;241m=\u001b[39mhistory_prompt,\n\u001b[0;32m   1832\u001b[0m     semantic_generation_config\u001b[38;5;241m=\u001b[39msemantic_generation_config,\n\u001b[0;32m   1833\u001b[0m     coarse_generation_config\u001b[38;5;241m=\u001b[39mcoarse_generation_config,\n\u001b[0;32m   1834\u001b[0m     codebook_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mcodebook_size,\n\u001b[0;32m   1835\u001b[0m     return_output_lengths\u001b[38;5;241m=\u001b[39mreturn_output_lengths,\n\u001b[0;32m   1836\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_coarse,\n\u001b[0;32m   1837\u001b[0m )\n\u001b[0;32m   1839\u001b[0m output_lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_output_lengths:\n",
      "File \u001b[1;32mc:\\Users\\salos\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bark\\modeling_bark.py:1185\u001b[0m, in \u001b[0;36mBarkCoarseModel.generate\u001b[1;34m(self, semantic_output, semantic_generation_config, coarse_generation_config, codebook_size, history_prompt, return_output_lengths, **kwargs)\u001b[0m\n\u001b[0;32m   1175\u001b[0m x_semantic_history, x_coarse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_histories(\n\u001b[0;32m   1176\u001b[0m     history_prompt\u001b[38;5;241m=\u001b[39mhistory_prompt,\n\u001b[0;32m   1177\u001b[0m     max_coarse_history\u001b[38;5;241m=\u001b[39mmax_coarse_history,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1181\u001b[0m     codebook_size\u001b[38;5;241m=\u001b[39mcodebook_size,\n\u001b[0;32m   1182\u001b[0m )\n\u001b[0;32m   1183\u001b[0m base_semantic_idx \u001b[38;5;241m=\u001b[39m x_semantic_history\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1185\u001b[0m semantic_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_semantic_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msemantic_output\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1187\u001b[0m n_window_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39mceil(max_generated_len \u001b[38;5;241m/\u001b[39m sliding_window_len))\n\u001b[0;32m   1189\u001b[0m total_generated_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "audio_array = model.generate(**inputs)\n",
    "audio_array = audio_array.cpu().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
